

\section{Discussion}

%In this section we address some of the challenges that we faced during the course of this project. We started 
%off by trying to use the off-the-shelf tools for this task. We started 
%by using Scala/Spark~\cite{ferc11} to benefit from the parallelization 
%performance there. Unfortunately Spark was not performant and the distributed reliability overhead 
%was much more than tolerable. We moved on to use Scala parallelization  framework itself, and unfortunately it did not satisfy our needs as well; there was 
%excessive memory overhead on map-reduce jobs. We migrated the core of the 
%system to Java Parallelism APIs and that was not good enough either, we still demanded 
%better. So we built our own parallel system which in actual performance had 
%the least of overhead, the least memory consumption and the most robust memory 
%model which avoided unpredictable CPU stalls on garbage collections in 
%processing the corpus.

Table~\ref{table:finalresultrecall} shows the distribution of extracted slot names.
The number of extraction between slot names vary greatly.
Some slots naturally have more results than other slots.
For example, DateOfDeath and CauseOfDeath have some of the fewest entities because only a few entities are deceased.
Some patterns use common words as as part of their patterns causing more extractions.
For example, Affiliate looks for common words like  \textit{and} and  \textit{with} as part of the pattern content.
These words are more common than \textit{dead}, \textit{died} or \textit{founded} in other patterns. 

%As part of future work regarding enhancing precision, we can focus on fixing the wrong entities found, remove noisy tags by the Lingpipe or use more accurate NLP taggers such as Stanford NLP (Due to the fact that Stanford NLP is very slow we avoided using it, as due to speed its use was out of question) and finally use better patterns to enahnce results matched by the patterns.
%On the other hand to increase recall we need to find better aliases and go for more resources to discover other ways that an entity might be addressed (e.g.\ twitter id, website, etc), add powerful patterns that can capture more cases of slot values. We would also use entity resolution methods and other advanced methods to improve 
%the accuracy and recall of entity extraction part. 

%For slot extraction, to improve the performance, we need: 1) Using multi-class classifiers instead of pattern matching method to extract slot values in order to increase both recall and accuracy for slots ``Affiliate'', ``AssociateOf'', ``FounderOf'', ``EmployeeOf'', ``FoundedBy'', ``TopMembers'', ``Contact\_Meet\_Entity'' and so on. 2) For special slots, like ``Titles'', ``DateOfDeath'', ``CauseOfDeath'', ``AwardsWon'', using different kind of advanced methods, e.g.\ classifiers, matching methods. 3) Using other NLP tools or using classifiers to overcome the drawbacks of the LingPipeâ€™s inaccurate tags. The first and second tasks are the most important tasks we need to do.

Some of the entities are popular and appear at a greater frequency in the data set.
For example, a `Corn Belt Power Cooperative' Google search results in 86,800 documents, where as `William H. Gates' returns 3,880,000 documents. 
%\ceg{Add an example of someone who is either much larger of smaller.}
We observed that more than half of the entities appear in less than 10 documents in the data set;
%\ceg{Can we add this figure - } 
a large portion have appeared only once. This magnificient change in coverage support the viability of our seach and filter schemes.

%A theory is that we are falling behind because we are using the cleansed (from HTML tags)
%version of the corpus. We believe there has been a reason that TREC has 
%included the actual HTML document as well as the cleansed version. This 
%definitely will convey some information to us. If it was as easy as reading 
%some clean text they wouldn't bother including so much data for teams to be 
%useless. So we guess is that we are missing some information from not using 
%the actual document. And, we are looking for tokens with entity value set 
%which will depend us dramatically on the accuracy of lingpipe, which is a fast 
%algorithm but is not as good as other NLP tools can be e.g. Stanford NLP.

% Note: Here we discuss why the results are the way they are
% Give pros and cons, talk about how the implemented algorithms
% performed in the actual implementation.
% Answer the `why' question about all the trends in the results section.





