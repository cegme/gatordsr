\begin{abstract}

% TODO this can be updated at the end

%In this paper we address extracting slot values from internet pertinent to entities in wikipedia, which is the most popular web-based, colaborative multilingual encyclopieda on the internet. This is comparable to Freebase in a query-driven manner. Our contributions are two fold, first we design a system to efficiently find central documents on the internet that contain directly citable content regarding a given wikipedia entity; second, we design a system to extract certain slot-values of the entity from those documents. Our results demonstrate that the system is very efficient in the web scale nature of the problem: being highly memory and I/O efficient and that we can achieve high accuracy and recall for given slot values.



Wikipedia.org is the largest online resource for free information and is maintained by a small number of volunteer editors.
The site contains 4.3 million english articles; these pages can easily be neglected, becoming out of date.
Any news-worthy event may require an update of several pages.
To address this issue of stale articles we create a system that reads
in a stream diverse web documents and recommends
facts to be added to specified Wikipedia pages.
We developed a three-stage streaming system that creates models of Wikipedia pages,
filters out irrelevant documents and 
extracts facts that are relevant to Wikipedia pages.
The systems is evaluated over a 500M page web corpus and 139 Wikipedia pages.
Our results show a promising framework for fast fact extraction from arbitrary web pages for Wikipedia.




\end{abstract}
