
\section{Evaluation}
\label{sec:results}

We evaluate the effectiveness of extracting slot values for 139 entities.
We look at the baseline coverage for entities and slot names we present in a 500M page snapshot of the English web.
We estimate the precision and recall of our extractions over several extracted fact. 

Our system was developed on a 32-core server described in Table~\ref{table:serverspec}.
Each document is annotated using a name entity extraction and in document coreference.
A bundle of documents are serialized into chunks and encrypted.
The total size of the data after compression and encryption is 4.5TB~\footnote{TREC Knowledge base acceleration stream corpus is available  \url{http://aws-publicdatasets.s3.amazonaws.com/trec/kba/index.html}}.
Data is ordered into 11952 date-hour buckets ranged from 2011-10-05-00 (5th of October 2011, 12am)
until 2013-02-13-23 (13th of Feburary 2013, 11pm).
The first four months of data (October 2011 - February 2012) is for training purposes, 
and we use this portion for rule and pattern creation and tuning.
The data set contains text from several web page types as listed in Table~\ref{table:documentsDist}.
%To have a sense of the scale of objects and compression as an example a 6mb gpg.xz files would become 45 mb thrift objects which can contain a couple of thousand StreamItems depending on their size.
%Some of the documents have null values for their annotation fields. The source code of our system is stored as an open source project where enthusiasts can also contribute to \cite{github}, also the relevant discussion mailing list is accessible here \cite{googlegroups}.
 
\begin{table}
\caption{Benchmark Server Specifications}
\centering
\label{table:serverspec}
\begin{tabular}{| c | p{4.8cm} |}
\hline 
\textbf{Spec} & \textbf{Details} \\ \hline
Model & Dell xxx 32 cores \\ \hline 
OS & CentOS release 6.4 Final \\ \hline 
Software Stack & GCC version 4.4.7, Java 1.7.0\_25, Scala 2.9.2, SBT 0.12.3 \\ \hline 
 RAM & 64GB\\ \hline 
 Drives & 2x2.7TB disks, 6Gbps, 7200RPM\\ \hline 
\end{tabular} 
\end{table}
 
\begin{table}
\caption{Document Chunks Distribution }
\centering
\label{table:documentsDist}

\begin{tabular}{|p{3.8cm}|p{3cm}|}
\hline 
\textbf{Document Type} & \textbf{\# of Documents}\\ 
\hline 
Arxiv & 10988  \\ \hline
 Classified & 34887  \\ \hline
 Forum & 77674  \\ \hline
 Linking & 12947  \\ \hline
 Mainstream News & 141936  \\ \hline
 Memetracker & 4137  \\ \hline
 News & 280629  \\ \hline
 Review & 6347 \\ \hline
 Social & 688848 \\ \hline
 Weblog & 740987  \\ \hline
\hline 
\end{tabular} 
\end{table}


 
 
 
We develop 172 extraction patterns covering each slot-name/entity-type combinations.
%Our final submission was named \textit{submission\_infer}. 
%Our results are as follows: Document extraction using query entity matching with aliases, sentence extraction using alias matching and co-reference.
%Slot extraction using patterns, NER tags and NP tags.
Out of the 500M documents and 139 entities we found 158,052 documents containing query entities, 17,885 unique extracted slot values for 8 different slots.
We did not get any results from 31 entities missing and 4 slots.

%On the performance of our initial submission run we performed random sampling via two processes, the results of which are according to Table~\ref{table:initialresult}.
In Table~\ref{table:initialresult} we performed two samples of a baseline and estimate the correctness of the extractions.
%\ceg{Add here how the baseline is different from the final submission}
The first was addressing the overall performance measures of the system, e.g.\ precision and recall.
The latter experiment was performed over an enhanced version of the system; we included the aliases from WP API, the alias generation process, and some additional patterns. 
We produced accuracies in range of  54\% and 55\%.
We classify the errors into two sets, an incorrect entities and incorrect extractions.
We found 15\% and 17\% incorrect entity names and we identified 27\% and 30\% incorrect value extracted across all entities and slot types.
The majority of errors were regarding poor slot value extraction patterns and incomplete aliases.

\begin{table}
\caption{Sampled accuracy of the results of the extracted facts.}
\centering
\label{table:initialresult}

\begin{tabular}{| c | c | p{2cm} | p{13mm} |}
\hline 
 & \textbf{Correct} & \textbf{Incorrect Entity name} & \textbf{Incorrect Value} \\ 
\hline 
Sampling \#1 & 55\% & 17\% & 27\% \\ 
\hline Sampling \#2 & 54\% & 15\% & 31\%  \\ 
\hline 
\end{tabular} 
\end{table}


%\ceg{Probably should add again here a reason why our method is different from the baseline.}
After enahncing the system via better and more extraction patterns  we provide more detailed statistics, as displayed in Table~\ref{table:finalresultrecall} and Table~\ref{table:finalresultaccuracy}.
Table~\ref{table:finalresultrecall} shows the recall for each slot name. 
Entities can have different coverages across the entire web, some were more popular (William H. Gates) or less well known such as (Stevens Cooperative School).
Similarly, slot names have various coverages for example \textit{Affiliate} is more probable across the entities when compared to \textit{AwardsWon}. %\ceg{(define `entity coverage' here)}
The slot name \textit{Affiliate} has was extracted the most number of times; \textit{AwardsWon} contained the next fewest with 38 instances found. 
,  affiliations of an organization the following cases have been enumerated \cite{tackbp}: 

An affiliate relationship can be defined in three general ways~\cite{tackbp}:
\begin{itemize}
\item  A relationship consisting solely of the two groups
interacting in a specific event context is not enough evidence to constitute a religious/political affiliation; 
\item Former political or religious affiliations are correct responses for this slot;
\item  Any relation that is not of parent-child form; a sub-organization is not an affiliate its parent organization but rather a Memberof.
\end{itemize}
Affiliate is a generic slot name; extracting affiliate relationships is difficult because the actual relationship must be determined.
Our patterns for this relationship led to noisy results.

However, less ambiguous slot names (AssociateOf) obtained higher accuracy but we have recall.
We developed patterns that explicitly expressed these relationships but we did not create enough patterns to 
express all forms of those slot names.


Table~\ref{table:finalresultaccuracy} addresses the relative accuracy measure per slot value.
\textit{AssociateOf} has the highest accuracy with 63.6\% and \textit{Affiliate}, \textit{Contact\_Meet\_PlaceTime} and \textit{EmployeeOf} have the lowest with lowest of 1\%, 1\% and 5\% accuracy respectively.


\begin{table}[t]
\caption{Recall Measure: Generic slot names like affiliate had the most recall, compared to less popular slot names e.g. DateOfDeath}
\centering
\label{table:finalresultrecall}
\begin{tabular}{|l|p{13mm}|p{22mm}|}
\hline 
 \textbf{Slot Name} & \textbf{Instances Found} & \textbf{Entity \hspace{5 mm} Coverage} \\ 
\hline 
Affiliate & 108598 & 80 \\ \hline 
AssociateOf & 25278 & 106 \\ \hline 
AwardsWon & 38 & 14 \\ \hline 
Contact\_Meet\_Entity & 191 & 8 \\ \hline 
Contact\_Meet\_PlaceTime & 5974 & 109 \\ \hline 
DateOfDeath & 87 & 14 \\ \hline 
EmployeeOf & 75 & 16 \\ \hline 
FoundedBy & 326 & 30 \\ \hline 
FounderOf & 302 &  29 \\ \hline 
Titles & 26823 & 118 \\ \hline 
TopMembers & 314 & 26 \\ \hline 

\end{tabular} 



\caption{Accuracy Measure: Accuracy of AffiliateOf was the best and Affiliate applied poorly due to ambiguity of being an affiliate of somebody/something}
\centering
\label{table:finalresultaccuracy}
\begin{tabular}{|l|p{10mm}|p{10mm}|p{11mm}|}
\hline 
 \textbf{Slot Name}  & \textbf{Correct} & \textbf{Wrong Entity} & {\small \textbf{Incorrect Value}} \\ 
\hline 
Affiliate & 1\% & 95\% & 5\% \\ \hline 
AssociateOf & 63.6\% & 9.1\% & 27.3\%  \\ \hline 
AwardsWon & 10\% & 10\% & 80\%  \\ \hline 
Contact\_Meet\_Entity & 21\% & 42\% & 37\%  \\ \hline 
Contact\_Meet\_PlaceTime & 5\% & 20\% & 85\%  \\ \hline 
DateOfDeath & 29.6\% & 71\% & 25\%  \\ \hline 
EmployeeOf & 5\% & 30\% & 65\%  \\ \hline 
FoundedBy & 62\% & 17\% & 21\%  \\ \hline 
FounderOf & 50\% & 0\% & 50\%  \\ \hline 
Titles & 55\% & 0\% & 45\%  \\ \hline 
TopMembers & 33\% & 17\% & 50\%  \\ \hline 

\end{tabular} 
\end{table}








% Note: Here is where we give as much stats as possible on our runs

