
\section{Evaluation}
\label{sec:results}

We evaluate the effectiveness of extracting slot values for 139 entities.
We look at the baseline coverage for entities and slot names we present in a 500M page snapshot of the English web.
We estimate the precision and recall of our extractions over several extracted fact. 

Our system was developed on a 32-core server described in Table~\ref{table:serverspec}.
Each document is annotated using a name entity extraction and in document coreference.
A bundle of documents are serialized into chunks and encrypted.
The total size of the data after compression and encryption is 4.5TB~\footnote{TREC Knowledge base acceleration stream corpus is available  \url{http://aws-publicdatasets.s3.amazonaws.com/trec/kba/index.html}}.
Data is ordered into 11952 date-hour buckets ranged from 2011-10-05-00 (5th of October 2011, 12am)
until 2013-02-13-23 (13th of Feburary 2013, 11pm).
The first four months of data (October 2011 - February 2012) is for training purposes, 
and we use this portion for rule and pattern creation and tuning.
The data set contains text from several web page types as listed in Table~\ref{table:documentsDist}.
%To have a sense of the scale of objects and compression as an example a 6mb gpg.xz files would become 45 mb thrift objects which can contain a couple of thousand StreamItems depending on their size.
%Some of the documents have null values for their annotation fields. The source code of our system is stored as an open source project where enthusiasts can also contribute to \cite{github}, also the relevant discussion mailing list is accessible here \cite{googlegroups}.
 
\begin{table}
\caption{Benchmark Server Specifications}
\centering
\label{table:serverspec}
\begin{tabular}{| c | p{4.8cm} |}
\hline 
\textbf{Spec} & \textbf{Details} \\ \hline
Model & Dell xxx 32 cores \\ \hline 
OS & CentOS release 6.4 Final \\ \hline 
Software Stack & GCC version 4.4.7, Java 1.7.0\_25, Scala 2.9.2, SBT 0.12.3 \\ \hline 
 RAM & 64GB\\ \hline 
 Drives & 2x2.7TB disks, 6Gbps, 7200RPM\\ \hline 
\end{tabular} 
\end{table}
 
\begin{table}
\caption{Document Chunks Distribution }
\centering
\label{table:documentsDist}

\begin{tabular}{|l|l|c|}
\hline 
\textbf{Document Type} & \textbf{\# of Documents} &  \textbf{Slots Found}\\ 
\hline 
Arxiv & 10988 & \\ \hline
 Classified & 34887 & \\ \hline
 Forum & 77674 & \\ \hline
 Linking & 12947 & \\ \hline
 Mainstream News & 141936 & \\ \hline
 Memetracker & 4137 & \\ \hline
 News & 280629 & \\ \hline
 Review & 6347 & \\ \hline
 Social & 688848 & \\ \hline
 Weblog & 740987 & \\ \hline
\hline 
\end{tabular} 
\end{table}


 
 
 
We develop 172 extraction patterns covering each slot-name/entity-type combinations.
%Our final submission was named \textit{submission\_infer}. 
%Our results are as follows: Document extraction using query entity matching with aliases, sentence extraction using alias matching and co-reference.
%Slot extraction using patterns, NER tags and NP tags.
Out of the 500 M documents and 139 entities we found 158,052 documents containing query entities, 17,885 unique extracted slot values for 8 different slots.
We did not get any results from 31 entities missing and 4 slots.

%On the performance of our initial submission run we performed random sampling via two processes, the results of which are according to Table~\ref{table:initialresult}.
In Table~\ref{table:initialresult} we performed two samples of a baseline submission and estimate the correctness of the extraction.
%\ceg{Add here how the baseline is different from the final submission}
We produced accuracies in range of  54\% and 55\%.
The errors present can be classified into two sets, incorrect entities and incorrect extractions.
We found 15\% and 17\% incorrect entity names and we identified 27\% and 30\% incorrect values extracted across all entities and slot types.
The majority of these errors were the result of poor extraction patterns and an incomplete alias set.

\begin{table}
\caption{Sampled accuracy of the results of the extracted facts.}
\centering
\label{table:initialresult}

\begin{tabular}{| c | c | p{2cm} | p{13mm} |}
\hline 
 & \textbf{Correct} & \textbf{Incorrect Entity name} & \textbf{Incorrect Value} \\ 
\hline 
Sampling \#1 & 55\% & 17\% & 27\% \\ 
\hline Sampling \#2 & 54\% & 15\% & 31\%  \\ 
\hline 
\end{tabular} 
\end{table}


%\ceg{Probably should add again here a reason why our method is different from the baseline.}
After the implementation of the baseline system, we developed an enhanced version. 
In this version we increased the number of extraction patterns and a generate a larger set of aliases.
More detailed statistics are available in Table~\ref{table:finalresultrecall} and Table~\ref{table:finalresultaccuracy}.
Table~\ref{table:finalresultrecall} shows the recall of each slot name. 
Entities can have different coverages across the entire web, some are more popular such as William H. Gates or less well known such as Stevens Cooperative School.
Similarly slot names have various coverages --- \textit{Affiliate} is more frequent compared to \textit{AwardsWon}. 
More sentences describing affiliate relationships exist across the WP entities than
sentences describing awards won. %\ceg{(define `entity coverage' here)}
The slot name \textit{Affiliate} has was extracted the most number of times and \textit{CauseOfDeath} was extracted the
fewest with 0 instances found matching the pattern list. \textit{AwardsWon} contained the next fewest with 38 instances found. 
Affiliate slot has a broad definition; any relation that directly connects an entity to another entity of any type can describe an Affiliate relationship.

%\begin{itemize}
%\item  any relation that is not of parent-child form, e.g. a sub-organization of an organization is not an affiliate of that organization but rather a member of.
%\item  A relationship consisting solely of the two groups
%interacting in a specific event context is not enough evidence to constitute a religious/political affiliation. 
%\item Former political or religious affiliations are correct responses for this slot. 
%\end{itemize}

To extract relationships such as affiliate is difficult because almost any
two items can be affiliated in some way.
%\ceg{Define the affiliate slot name.}
Our patterns for this relationship led to noisy results.
On the other hand for more precise slot names our accuracy increases but we have less recall.
Table~\ref{table:finalresultaccuracy} addresses the relative accuracy measure per slot value.
\textit{AssociateOf} has the highest accuracy with 63.6\% and  \textit{Affiliate}, \textit{Contact\_Meet\_PlaceTime} and \textit{EmployeeOf} have the lowest with lowest of 1\%, 1\% and 5\% accuracy respectively.
A discussion of different slots is available in literature \cite{tackbp}.


\begin{table}
\caption{Recall Measure: Generic slot names like affiliate had the most recall, compared to less popular slot names e.g. DateOfDeath}
\centering
\label{table:finalresultrecall}
\begin{tabular}{|l|p{13mm}|p{22mm}|}
\hline 
 \textbf{Slot Name} & \textbf{Instances Found} & \textbf{Entity \hspace{5 mm} Coverage} \\ 
\hline 
Affiliate & 108598 & 80 \\ \hline 
AssociateOf & 25278 & 106 \\ \hline 
AwardsWon & 38 & 14 \\ \hline 
CauseOfDeath & 0 & 0 \\ \hline 
Contact\_Meet\_Entity & 191 & 8 \\ \hline 
Contact\_Meet\_PlaceTime & 5974 & 109 \\ \hline 
DateOfDeath & 87 & 14 \\ \hline 
EmployeeOf & 75 & 16 \\ \hline 
FoundedBy & 326 & 30 \\ \hline 
FounderOf & 302 &  29 \\ \hline 
Titles & 26823 & 118 \\ \hline 
TopMembers & 314 & 26 \\ \hline 

\end{tabular} 



\caption{Accuracy Measure: Accuracy of AffiliateOf was the best and Affiliate applied poorly due to ambiguity of being an affiliate of somebody/something}
\centering
\label{table:finalresultaccuracy}
\begin{tabular}{|l|p{10mm}|p{10mm}|p{11mm}|}
\hline 
 \textbf{Slot Name}  & \textbf{Correct} & \textbf{Wrong Entity} & {\small \textbf{Incorrect Value}} \\ 
\hline 
Affiliate & 1\% & 95\% & 5\% \\ \hline 
AssociateOf & 63.6\% & 9.1\% & 27.3\%  \\ \hline 
AwardsWon & 10\% & 10\% & 80\%  \\ \hline 
CauseOfDeath & 0\% & 0\% & 0\%  \\ \hline 
Contact\_Meet\_Entity & 21\% & 42\% & 37\%  \\ \hline 
Contact\_Meet\_PlaceTime & 5\% & 20\% & 85\%  \\ \hline 
DateOfDeath & 29.6\% & 71\% & 25\%  \\ \hline 
EmployeeOf & 5\% & 30\% & 65\%  \\ \hline 
FoundedBy & 62\% & 17\% & 21\%  \\ \hline 
FounderOf & 50\% & 0\% & 50\%  \\ \hline 
Titles & 55\% & 0\% & 45\%  \\ \hline 
TopMembers & 33\% & 17\% & 50\%  \\ \hline 

\end{tabular} 
\end{table}








% Note: Here is where we give as much stats as possible on our runs

