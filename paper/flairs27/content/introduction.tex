
\section{Introduction}


In this paper we develop an efficient query-driven slot value extraction for given Wikipedia.org (WP) entities from a timely stream of documents on internet as they are being created. Slot value extraction is formally defined as follows: match each sentence to the generic sentence structure of [\textit{Subject} - \textit{Verb} - \textit{Adverbial/Complement}]\cite{sentencePatterns08}, where  \textit{Subject} represents the entity and \textit{Verb} is the relation type we are interested in (e.g. Table~\ref{table:slotNameOntology}). If a sentence matches these two components of the sentence pattern, \textit{Adverbial/Complement} would be returned as the other side of the relation (which we refer to as slot value). Slot value extraction is a challenging task in current state of the art Knowldge Bases (KB). Popular graphical KB such as Freebase or DBPedia keep data in structured format where entities are connected via relationships (\textit{Verb}) and the associated attributes (\textit{Adverbial/Compelement}). Our system can be used to automatically populate such KBs or even fill-in the information boxes at entity WP page itself.

Entities from three categories of \textit{person}, \textit{organization} and \textit{facility} are considered. The challenges that we address are: first finding documents that contain useful information about the entity (avoid spams, documents that do not have direct information about the entity even though they mention them, etc), the scale where \textit{stream processing} nature of the system makes it suitable to avoid batch processing natures of Hadoop and operate in the realm of streams such as twitter storm\footnote{http://storm-project.net/} which similarly processes unblounded streams of twitter data or Spark Streaming \footnote{http://spark.incubator.apache.org/}. Further, this is called streaming because data is being generated as time goes
on and for each extraction we should only consider current or past data. As other Natural Language Processing tasks, precision and recall of the extent we can extract slot values are very important metrics that we will discuss later on. Having to balance between infamous and obscure enitities, dealing with slots that can be very broad (e.g. things that an entity is affiliated with) or very specific (such as cause of death of a \textit{person} entity).

To take on an example assume we are analyzing a sentence from the internet such as "\underline{Boris Berezovsky} made his fortune in Russia in the 1990s when the country went through privatisation of state property and 'robber capitalism', and passed away March 2013.". First we have to pay attemtion that there are two \textit{Boris Berezovsky} entities in WP, one a businessman and the other a pianist. Any slot value extraction shall take this into account and try to come up with a viable distinguishing policy (Entity Resolution). Then, we match the sentence to find a slot value such as \textit{DateOfDeath} valued at March 2013. Other examples that this system can answer could be 'Who a person has met during a certain period of time?', 'Who are the employees of this organization?', 'Who has met who in this facility at a certain time?'.

 An important challenge in maintaining WP, the most popular web-based, collaborative, multilingual KB on the internet, is making sure its contents are up-to-date. Presently, there is considerable time lag between the publication date of cited news and the date of an edit to WP creating the citation. The median time lag for a sample of about 60K web pages cited by WP articles in the \textit{living\_people} category is over a year and the distribution has a long and heavy tail \cite{JFrank12}. Also, the majority of WP entities have updates on their associated article much less frequently than their mention frequency. Such stale entries are the norm in any large KB because the number of humans maintaining the KB is far fewer than the number of entities. %Further, the number of mentions is much larger than the number of entities~\cite{JFrank12}. 
Reducing latency keeps KBs and WP relevant and helpful to its users. Given an entity page, possible citations may come from a variety of sources. The actual citable information is a small percentage of the total documents that appear on the web. We develop a system to read streaming data and filter out articles that are candidates for citations. 

Our system contains three main components. First, we pre-process the data and build models representing the KB entries. Next, we use the models to filter a stream of documents so they only contain candidate citations. Lastly, we processes sentences from candidate extractions and return slot values. 
% EITHER SAY WHAT IT MEANS OR GIVE EXAMPLES OR COMMENT IT
%We build a modular system that allows us to explore the nuances of the training data and queries. 
Overall, we contribute the following:
\begin{itemize}[noitemsep,nolistsep]
\item Introduce a method to build models of name variations
\item Built a system to filter a large amount of diverse documents
\item Extract, infer and filter entity-slot-value triples of information to be added to KB 
\end{itemize}

\begin{table}
\caption{Ontology of Slots }
\centering
\label{table:slotNameOntology}

\begin{tabular}{|c|c|c|}
\hline 
\textbf{PER} & \textbf{FAC} & \textbf{ORG} \\ 
\hline 
\begin{tabular}{@{}l@{}}Affiliate \\ AssociateOf \\ Contact\_Meet\_PlaceTime \\ AwardsWon \\ 
DateOfDeath \\ 
CauseOfDeath\\
Titles\\
FounderOf\\
EmployeeOf\end{tabular}
  &
   \begin{tabular}[b]{l}Affiliate \\ Contact\_Meet\_Entity \end{tabular} 
   & 
   \begin{tabular}{@{}l@{}}Affiliate \\ TopMembers \\ FoundedBy\end{tabular} \\ 
\hline 
\end{tabular} 
\end{table}
% Evaluation Criteria


% Quick Discussion of our approach

