

\section{Implementation}

We extract aliases for entities from Wikipedia automatically both using API 
and using the actual page content, then apply pattern matching rules for slot 
value extraction. 

\subsection{Alias Generation}

We use Wikipedia API to get some aliases automatically. This is done by 
retrieving backlink references (redirects of a wiki entity). Unfortunately 
this is not good enough and to enhance recall we need more aliases. To have 
better use of a wiki page we parse HTML DOM of the page, then use regular 
expressions to extract the bold phrases of the first paragraph as alias of the 
actual entity. Based on our observation this is a very accurate heuristic and 
provides us with lots of famous aliases of the entities. To consider other 
typical cases we consider some generic first name last name order swapping 
conventions such as Bill Gates -\textgreater Gates, Bill.  

\subsection{Pattern Matching}

In this system, we use pattern matching methods to find corresponding slot 
values for entities. The procedure is: 1) Find stream items that contain the 
entities by using alias names of the entities. 2) Inside these stream items, 
fetch the sentences which contain entities by using alias names and the 
coreference information provided by Lingpipe tags. 3) Use these sentences to 
match existing patterns. 4) When patterns matched, generate SSF results.

The format of the patterns is consist of five fields:
1) the type of the entity. 
2) the slot name.
3) the pattern content.
4) the direction of the slot value to the entity.
5) the type of the slot values. When we match one pattern, we match all the 
fields except the third field. The type of slot values could be the entity 
type tagged by Lingpipe, noun phrase tagged by OpenNLP and the time phrases we 
hard-code. For these three kinds of patterns, we implement them in different 
ways accordingly. 

Next we will explain the patterns with more details. Lets’s first look at one 
example of the first kind of patterns. For slot FounderOf, we have this
pattern: \textless PER, FounderOf, founder, right, ORG\textgreater. PER means 
that the entity we are finding slot values for is a PER entity; FounderOf 
means this is a pattern for FounderOf slot. Founder is the anchor word we are 
trying to match in the sentence text; right means that we are going to the 
right part of the sentence to match the pattern and find the slot value; ORG 
means the slot value should be a ORG entity.

Here is an example for the second type: \textless PER, AwardsWon, awarded, 
right, NP\textgreater, NP meaning noun phrase. This pattern can be interpreted 
as that we are looking for a noun phrase after the “awarded” since that noun 
phrase could possibly represent an award. Since titles and awards are usually 
not the Lingpipe entities, we use the OpenNLP noun phrase chunker to fetch the 
noun phrases.

Another example for the third type: \textless PER, DateOfDeath, died, right, 
last night\textgreater. In this pattern, “last night” means we are looking for 
exactly the phrase “last night” to the right of “died”. This pattern is 
inspired by the intuition that in news articles, people often mention that 
somebody died last night instead of mentioning the accurate date information 
and Lingpipe tends not to tag phrases like “last night” as a DATE entity. 

To improve recall, we introduce generic patterns that can generate many results.
But these patterns generate many errors, too. Thus there is a trade-off 
between the accuracy and recall. This is one drawback of using pattern 
matching method, meaning it is really hard to find good patterns with both 
high accuracy and recall. 

For accuracy, we have found that there are three major kinds of errors: 1) 
wrong entities found; 2) wrong tags by the Lingpipe; 3) wrong results matched 
by the patterns. We solve the first problem by using the wikipedia or twitter 
information of the entities to get better alias names. And we use
post- processing to reduce the second and third types of errors.

\subsection{Post Processing}

The SSF output of many extractions is noisy. The data contains duplicates and 
incorrect extractions. We can define rules to sanitize the output only using 
the information present in the SSF file. The file is processed in time order, 
in a tuple-at-a-time fashion to minimize the impact on accuracy. We define 
two classes of rules deduplication rules and inference rules.

The output contains many duplicate entries. As we read the list of extrated 
slots we create rules to define "duplicate". Duplicates can be present in a 
window of rows; we use a window size of 2 meaning we only be adjacent rows. 
Two rows are duplicates if 1) they have the same exact extraction, 2) if the 
rows have the same slot name and a similar slot value and 3) if the extracted 
sentence for a particular slot types come from the same sentence.

 New slots can be deduced from existing slots by defineing inference rules. 
 For example, two slots for the task are "FounderOf" and "FoundedBy". A safe 
 assumtion is these slot names are biconditional logical connectives with the 
 entities and slot values. Therefore, we can express a rule "X FounderOf Y" 
 equals "Y FoundedBy X" where X and Y are single unique entities. Additionally,
 we found that the slot names "Contact\_Meet\_PlaceTime" could be infered as
 "Contact\_Meet\_Entity" if the Entity was a FAC and the extracted sentence 
 contained an additional ORG/FAC tag.  
We also remove erronious slots that have extractions that are several pages in 
length or tool small. Errors of extracting long sentences can typically be 
attributed to poor sentence parsing of web documents. We have some valid
"small" extractions. For example a comma may separate a name and a title
(e.g. "John, Professor at MIT"). But such extraction rules can be particularly 
noisy, so we check to see if the extracted values have good entity values.

The aim in post processing was to sanitize the results increasing the 
precision of the results by eliminating spurrious results.

\subsection{Shortcomings}

The missing twitter entities are more than the 50\% of the twitter entities we 
need to work on and this is a very very bad performance. One reason could be 
that those entities are not very famous. For example Brenda Weiler google 
search result is 860,000 documents our of billions of web documents. For our 
small portion of the web it might make sense. If you pay attention to the 
histogram of the entities found you'll note that more than half of the 
entities have appeared in less than 10 StreamItems. A good portion have 
appeared only once. Which the document does not necessarily have good 
information for us.

A theory is that we are falling behind because we are using the cleansed 
version of the corpus. We believe there has been a reason that TREC has 
included the actual HTML document as well as the cleansed version. This 
definitely will convey some information to us. If it was as easy as reading 
some clean text they wouldn't bother including so much data for teams to be 
useless. So we guess is that we are missing some information from not using 
the actual document. And, we are looking for tokens with entity value set 
which will depend us dramatically on the accuracy of lingpipe, which is a fast 
algorithm but is not as good as other NLP tools can be e.g. Stanford NLP.

% Note: Talk about how each of the algorithms were created

% Note: Give enough information for our algorithms to be
% reimplemented and verified.

