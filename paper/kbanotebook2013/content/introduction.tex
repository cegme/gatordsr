
\section{Introduction}


An important challenge in maintaining Wikipedia.org (WP), the most popular 
web-based, collaborative, multilingual encyclopedia in the internet, is to 
make sure its contents are up-to-date. Presently, there is considerable time lag 
between the publication date of cited news and the date of an edit to WP 
creating the citation. The median time lag for a sample of about 60K
web pages cited by WP articles in the \textit{living\_people} category is over 
a year and the distribution has a long and heavy tail~\cite{JFrank12}. 
Also, the majority of WP entities have updates on their associated article much 
less frequently than their mention frequency. Such stale entries 
are the norm in any large knowledge base (KB) because the number of humans 
maintaining the KB is far fewer than the number of entities. 
%Further, the number of mentions is much larger than the number of entities~\cite{JFrank12}. 
Reducing latency keeps KBs such as WP relevant and helpful it users.

Given an entity page, possible citations may come from a variety of sources.
The actual citable information is a small percentage of the total documents that appear on the web.
We develop a system to read streaming data and filter out articles that are candidates for citations.
Given these documents, we extract the pairs of information in the 
article that is recommended for citations for each knowledge base page.
\ceg{add details}

Our system contains three main components. First, we pre-process the data and
build models representing the knowledge base entries.
Next, we use the models to filter a stream of documents so they only contain 
candidate recommendations.
Lastly, we processes sentences from candidate extractions and return 
specific information to be cited.
\ceg{add details}


We built this system as part of a formal task described in Section~\ref{sec:kbatask}.
The data set from this task drop most our our design systems.
Overall, our contributions are the following:
\begin{itemize}[noitemsep,nolistsep]
\item we introduce a method to build models of name variations (Section~\ref{section:aliasgeneration});
\item we built a system to filter a large amount of diverse documents (Section~\ref{sec:ccrimpl});
\item we extract entity-slot-value triples of information to be added to KB (Section~\ref{sec:ssve});
\item we filter the final results using deduplication and inference (Section~\ref{section:highAccuracyFilter});
\item we self-evaluate our results over a 4.5 TB data set (Section~\ref{sec:results}).
\end{itemize}


\section{KBA Task Background}
\label{sec:kbatask}
The National Institute of Standards (NIST) hosted the
Text REtrieval Conference (TREC) --- Knowledge Base Acceleration challenge in 2013. The task
contains two main sections designed for this track, Cumulative Citation Recommendation 
and Streaming Slot Filling. Due to the importance of knowledge bases, both of these 
tracks aim to accelerating populating them, hence the title Knowledge Base Acceleration (KBA).
Below we describe each of these tracks and their purposes.

\subsection{Cumulative Citation Recommendation (CCR)}

For this track, assessors were instructed to ``use the Wikipedia article to 
identify (disambiguate) the entity, and then imagine forgetting all info in the WP 
article and asking whether the text provides any information about the entity'' \cite{JFrank12}.
Documents are divided according if an entity is mentioned and a relevance level to the entity.

More specifically, a document may have a mention or be without a mention.
\begin{itemize}[noitemsep]
  \item \textbf{Mention:} Document explicitly mentions target entity, such as full 
    name, partial name, nickname, pseudonym, title, stage name.
  \item \textbf{Zero-mention:} Document does not directly mention target. Could 
    still be relevant, e.g.\ metonymic references like ``this administration'' 
    $\rightarrow$ ``Obama''. See also synecdoche. A document could also be relevant to 
    target entity through relation to entities mentioned in document -- apply this 
    test question: can I learn something from this document about target entity using 
    whatever other information I have about entity?
\end{itemize}

The relevance of a document to a query is split into the four classifications.
\begin{itemize}[noitemsep]
  \item \textbf{Garbage:} not relevant, e.g.\ spam.
  \item \textbf{Neutral:} Not relevant, i.e.\ no info could be deduced about entity, 
    e.g., entity name used in product name, or only pertains to community of target 
    such that no information could be learned about entity, although you can see how 
    an automatic algorithm might have thought it was relevant.
  \item \textbf{Relevant:} Relates indirectly, e.g., tangential with substantive 
    implications, or topics or events of likely
    impact on entity.
  \item \textbf{Central:} Relates directly to target such that you would cite it in 
    the WP article for this entity, e.g.\ entity is a
    central figure in topics/events.
\end{itemize}


\subsection{Streaming Slot Filling (SSF)}
The task is that given certain WP or Twitter entities 
(wiki/twitter URLs) and certain relations of interest (given in
Table~\ref{table:slotNameOntology}), extract as many triple relations as possible (
hence, slot filling). This can be used to automatically populate knowledgebases 
such as free-base or DBPedia or even fill-in the information boxes at Wikipedia. 
Below, you can view some examples of what it means to fill in a slot value; in 
each example there is a sentence of interest that we wish to extract slot values 
from, an entity that the slot value is related to, and a slot name which can be 
thought of as the topic of the slot value:

\noindent \textbf{Example 1:} ``Matthew DeLorenzo and Josiah Vega, both 14 years old and students 
at Elysian Charter School, were honored Friday morning by C-SPAN and received 
\$1,500 as well as an iPod Touch after winning a nationwide video contest.''

Entity:  http://en.wikipedia.org/wiki/Elysian\_Charter\_School

Slot name: Affiliate

Possible slot values: ``Matthew DeLorenzo'', ``Josiah Vega''

Incorrect slot values: ``C-SPAN'', ``iPod Touch''

\noindent \textbf{Example 2:} ``Veteran songwriters and performers Ben Mason, Jeff Severson and 
Jeff Smith will perform on Saturday, April 14 at 7:30 pm at Creative Cauldron 
at ArtSpace, 410 S. Maple Avenue.''

Entity: http://en.wikipedia.org/wiki/Jeff\_Severson

Slot name: Affiliate

Possible slot values: ``Ben Mason'', ``Jeff Severson'', ``Jeff Smith''

Incorrect slot values: ``Creative Caldron'', ``Art Space''

\noindent \textbf{Example 3:}  ``Lt. Gov. Drew Wrigley and Robert Wefald, a retired North Dakota 
district judge and former state attorney general, unveiled the crest Friday 
during a ceremony at the North Dakota Capitol.''

Entity: http://en.wikipedia.org/wiki/Hjemkomst\_Center

Slot name: Contact\_Meet\_PlaceTime

Slot value: ``Friday during a ceremony at the North Dakota Capitol''


\noindent In \textit{streaming} slot filling, we are only interested in new 
slot values that were not substantiated earlier in the stream corpus.
In Table~\ref{table:slotNameOntology} you can view some slot values 
and their types, the comprehensive description of which can be found at
\cite{tackbp} and \cite{aec}. The details of the metric for SSF will favor systems 
that most closely match the changes in the ground truth time line of slot values. 
This is done by searching for other documents that mentioned an entity and exactly 
matched the slot fill strings selected by the assessors.



In the rest of the paper, we address the following material. In Section 2, we 
sketch the main components of the system and their purposes. In Section 3, 
describe the details of how we designed and implemented certain components 
critical to the behavior of the system. Section 4 addresses the performance and 
the results we achieved. Section 5, goes through a discussion of the challenges we 
had to produce reliable results over the massive amount of information available, 
and how we overcome these issues. For similar approaches regarding last year's 
track you can refer to \cite{ji2011knowledge}.

%\ceg{Add a discussion of why machine driven kb population is important.}


\begin{table}[b]
\caption{Ontology of Slot Name Categories }
\centering
\label{table:slotNameOntology}

\begin{tabular}{|c|c|c|}
\hline 
\textbf{PER} & \textbf{FAC} & \textbf{ORG} \\ 
\hline 
\begin{tabular}{@{}l@{}}Affiliate \\ AssociateOf \\ Contact\_Meet\_PlaceTime \\ AwardsWon \\ 
DateOfDeath \\ 
CauseOfDeath\\
Titles\\
FounderOf\\
EmployeeOf\end{tabular}
  &
   \begin{tabular}[b]{l}Affiliate \\ Contact\_Meet\_Entity \end{tabular} 
   & 
   \begin{tabular}{@{}l@{}}Affiliate \\ TopMembers \\ FoundedBy\end{tabular} \\ 
\hline 
\end{tabular} 
\end{table}


%\ceg{How will each submission be evaluated?}

% Motivation


% KBA Task


% Evaluation Criteria


% Quick Discussion of our approach

